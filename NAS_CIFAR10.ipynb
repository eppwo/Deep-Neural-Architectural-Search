# -*- coding: utf-8 -*-
"""
Created on Sat Jan  1 13:58:14 2022

@author: emili
"""

# -*- coding: utf-8 -*-
"""
Created on Sat Jan  1 13:56:18 2022

@author: emili
"""

# -*- coding: utf-8 -*-
"""ResNet152ForComparison

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XmBhS-eZt4DZx6c-RnbPzxMopM6nsib1
"""

# Commented out IPython magic to ensure Python compatibility.
# -*- coding: utf-8 -*-
"""
Created on Sat Nov 13 17:05:28 2021

@authors: Emilie P. P. W. Olesen - s164060 & Rebekka D. Beneke - s153805

Based on https://www.youtube.com/watch?v=DkNIBBBvcPs and https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py
"""

# Install tensorboard for data visualization
#!pip install tensorboard


# Environmental variable
CUDA_LAUNCH_BLOCKING=1

# Import packages 
import sys
#import tensorflow as tf
import datetime, os
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
from torch.utils.data import DataLoader, random_split, Subset
from torch.nn.parameter import Parameter
from torchvision.datasets import MNIST
from torchvision.transforms import ToTensor
from torchvision import transforms
from tqdm.autonotebook import tqdm
from sklearn.metrics import classification_report
import time
import matplotlib.pyplot as plt
from tqdm import tqdm
import numpy as np
from torch.distributions import Categorical, Normal
#import torch.utils.tensorboard as tb
torch.set_printoptions(linewidth=120)
#from torch.utils import tensorboard
#from torch.utils.tensorboard import SummaryWriter
#from tensorboard import notebook
# %pylab inline
# %load_ext tensorboard


from dask.distributed import Client

if __name__ == '__main__':
    client = Client()

    # Settings and data
    # Device

    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    
    
    # Hyperparameters
    random_seed = 123
    learning_rate = 3e-4 #0.01
    num_epochs = 20
    batch_size = 128
    num_epochs_NAS = 10 # This is the number of epochs we wish to update our network variables in the NAS.
    
    # Architecture
    num_classes = 2
    img_channels = 3
    
    
    class block(nn.Module):
        def __init__(self, in_channels, out_cannels, identity_downsample = None, stride = 1):
            super(block, self).__init__()
            self.explansion = 4 
            self.conv1 = nn.Conv2d(in_channels, out_cannels, kernel_size = 1, stride = 1, padding = 0)
            self.bn1 = nn.BatchNorm2d(out_cannels)
            self.conv2 = nn.Conv2d(out_cannels, out_cannels, kernel_size = 3, stride = stride, padding = 1)
            self.bn2 = nn.BatchNorm2d(out_cannels)
            self.conv3 = nn.Conv2d(out_cannels, out_cannels*self.explansion, kernel_size = 1, stride = 1, padding = 0)
            self.bn3 = nn.BatchNorm2d(out_cannels*self.explansion)
            self.relu = nn.ReLU()
            self.identity_downsample = identity_downsample
            
        def forward(self, x):
            identity = x
            
            x = self.conv1(x)
            x = self.bn1(x)
            x = self.relu(x)
            x = self.conv2(x)
            x = self.bn2(x)
            x = self.relu(x)
            x = self.conv3(x)
            x = self.bn3(x)
            
            if self.identity_downsample is not None:
                identity = self.identity_downsample(identity)
                
            x += identity
            x = self.relu(x)
            return x
        
    class ResNet(nn.Module):  # [3, 4, 6, 3]
        def __init__(self, block, layers, image_channels, num_classes):
            super(ResNet, self).__init__()
            self.in_channels = 64
            self.conv1 = nn.Conv2d(image_channels, 64, kernel_size = 7, stride = 2, padding = 3)
            # Nedenstående kan implementeres, hvis vi vil teste udfaldet af forskellige conv lag. Valget af conv lag skal så afhænge af størrelsen på input som skal tilføjes ovenfor i initialiseringen kernel_size 
            #if kernel_size = 3
              #self.conv1 = nn.Conv2d(image_channels, 64, kernel_size = 7, stride = 2, padding = 3)
            #if kernel_size = 5
              #self.conv1 = nn.Conv2d(image_channels, 64, kernel_size = 7, stride = 2, padding = 3)
            #if kernel_size = 7
              #self.conv1 = nn.Conv2d(image_channels, 64, kernel_size = 7, stride = 2, padding = 3)
            self.bn1 = nn.BatchNorm2d(64)
            self.relu = nn.ReLU()
            self.maxpool = nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)
            
            # ResNet layers
            self.layer1 = self._make_layer(block, layers[0], out_channels = 64, stride = 1)
            self.layer2 = self._make_layer(block, layers[1], out_channels = 128, stride = 2)
            self.layer3 = self._make_layer(block, layers[2], out_channels = 256, stride = 2)        
            self.layer4 = self._make_layer(block, layers[3], out_channels = 512, stride = 2)
            
            self.avgpool = nn.AdaptiveAvgPool2d((1,1))
            self.fc = nn.Linear(512*4, num_classes)
            
        def forward(self, x):
            x = self.conv1(x)
            x = self.bn1(x)
            x = self.relu(x)
            x = self.maxpool(x)
      
    
            x = self.layer1(x)
            x = self.layer2(x)        
            x = self.layer3(x)
            x = self.layer4(x)
            
            x = self.avgpool(x)
            x = x.reshape(x.shape[0], -1)
            x = self.fc(x)
            return x
    
                
                
        def _make_layer(self, block, num_residual_blocks, out_channels, stride):
            identity_downsample = None
            layers = []
            
            if stride != 1 or self.in_channels != out_channels * 4:
                identity_downsample = nn.Sequential(nn.Conv2d(self.in_channels, out_channels*4, kernel_size = 1, stride = stride,), nn.BatchNorm2d(out_channels*4))
            
            
            layers.append(block(self.in_channels, out_channels, identity_downsample, stride))
            self.in_channels = out_channels * 4
            
    
            for i in range(num_residual_blocks - 1):
                layers.append(block(self.in_channels, out_channels))
                
                return nn.Sequential(*layers)
    
    
    def ResNet18(img_channels, num_classes):
        return ResNet(block, [2, 2, 2, 2], img_channels, num_classes)
    
    
    def ResNet50(img_channels, num_classes): 
        return ResNet(block, [3, 4, 6, 3], img_channels, num_classes)
    
    def ResNet101(img_channels, num_classes):
        return ResNet(block, [3, 4, 23, 3], img_channels, num_classes)        
    
    def ResNet152(img_channels, num_classes):
        return ResNet(block, [3, 4, 36, 3], img_channels, num_classes)
    
    # Load CIFAR10 data:
    transform = transforms.Compose(
        [transforms.ToTensor(),
         transforms.Normalize((0.5, 0.5, 0.5),(0.5, 0.5, 0.5))
        ]
    )
    
    # Load dataset
    trainset = torchvision.datasets.CIFAR10(root='./data', train=True,
                                            download=True, transform=transform)
    testset = torchvision.datasets.CIFAR10(root='./data', train=False,
                                           download=True, transform=transform)
    
    classes = ('plane', 'car', 'bird', 'cat', 'deer',
               'dog', 'frog', 'horse', 'ship', 'truck')
    
    used_categories = range(len(classes))
    ## USE CODE BELOW IF YOUR COMPUTER IS TOO SLOW
    reduce_dataset = True
    if reduce_dataset:
        used_categories = (3, 5) # cats and dogs
    
        classes = [classes[i] for i in used_categories]
        new_train_data = []
        new_train_labels = []
    
        new_test_data = []
        new_test_labels = []
        for i, t in enumerate(used_categories):
            new_train_data.append(trainset.data[np.where(np.array(trainset.targets) == t)])
            new_train_labels += [i for _ in range(new_train_data[-1].shape[0])]
    
            new_test_data.append(testset.data[np.where(np.array(testset.targets) == t)])
            new_test_labels += [i for _ in range(new_test_data[-1].shape[0])]
    
        new_train_data = np.concatenate(new_train_data, 0)
        trainset.data = new_train_data
        trainset.targets = new_train_labels
    
        new_test_data = np.concatenate(new_test_data, 0)
        testset.data = new_test_data
        testset.targets = new_test_labels
    
    
    
    train_dl = torch.utils.data.DataLoader(trainset, batch_size=4,
                                              shuffle=True, num_workers=2)
    test_dl = torch.utils.data.DataLoader(testset, batch_size=4,
                                             shuffle=True, num_workers=2)
    train_data_iter = iter(train_dl)
    test_data_iter = iter(test_dl)
    print('used classes:', classes)
    # Monitoring training time of the net
    
    
    # Attempt from https://github.com/rasbt/stat453-deep-learning-ss21/blob/main/L14/2-resnet-example.ipynb
    
    torch.manual_seed(random_seed)
    #optimizer = torch.optim.Adam(NAS_model.parameters(), lr=learning_rate)
    
    def compute_accuracy(NAS_model, data_loader):
      correct_pred, num_examples = 0, 0
      for i, (features, targets) in enumerate(data_loader):            
          features = features.to(device)
          targets = targets.to(device)
          logits = NAS_model(features)
          _, predicted_labels = torch.max(logits, 1)
          num_examples += targets.size(0)
          correct_pred += (predicted_labels == targets).sum()
      return correct_pred.float()/num_examples * 100
    
    # Commented out IPython magic to ensure Python compatibility.
    ## Def for traning model:
    
    def trainmodel(NAS_model, optimizer):
      start_time = time.time()
      training_accuracy_array = []
      test_accuracy_array = []
      train_loss = []
      test_loss = []  
    
      #  # Save information from sampled architectures; by initializing empty dictionary
      # architecture = {'Train loss': [], 'Test loss': [], 'Training accuracy': [], 'Test accuracy': []}
    
      for epoch in range(num_epochs):
          NAS_model = NAS_model.train()
    
    
          #logdir = os.path.join("logs", datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))
         # tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)
    
          #print("Entering training mode")
          for batch_idx, (features, targets) in enumerate(train_dl):
            features = features.to(device)
            targets = targets.to(device)
            
            ### FORWARD AND BACK PROP for training
            logits = NAS_model(features)
            cost = torch.nn.functional.cross_entropy(logits, targets)
            train_loss.append(cost)
            optimizer.zero_grad()
            cost.backward()
            
            ### UPDATE MODEL PARAMETERS
            optimizer.step()
                
          # ### LOGGING
          # if epoch % 1 == 0:
                # print ('Epoch: %03d/%03d | Train cost: %.4f' 
                #       %(epoch+1, num_epochs, cost))
                # print ('Epoch: %03d/%03d' 
                #        %(epoch+1, num_epochs))
    
            # Leave training enter evaluation mode
    
          #print("Entering evaluation mode")
          NAS_model =NAS_model.eval() # eval mode to prevent upd. batchnorm params during inference
    
          for batch_idx, (features, targets) in enumerate(test_dl):
          
            features = features.to(device)
            targets = targets.to(device)
            
            ### FORWARD AND BACK PROP for training
            logits = NAS_model(features)
            test_cost = torch.nn.functional.cross_entropy(logits, targets)
            test_loss.append(test_cost)
            optimizer.zero_grad()
            
            test_cost.backward()
            
            ### UPDATE MODEL PARAMETERS
            optimizer.step()
            
      #       ### LOGGING
          if epoch % 1 == 0:
            # print ('Epoch: %03d/%03d | Test cost: %.4f' 
            #         %(epoch+1, num_epochs, test_cost))
            # print ('Epoch: %03d/%03d' 
            #          %(epoch+1, num_epochs))
    
            with torch.set_grad_enabled(False): # save memory during inference
              # Compute accuracy
              training_accuracy = compute_accuracy(NAS_model, train_dl)
              test_accuracy = compute_accuracy(NAS_model, test_dl)
              train_loss.append(cost)
              test_loss.append(test_cost)
              print('Epoch: %03d/%03d training accuracy: %.2f%%' % (epoch+1, num_epochs, compute_accuracy(NAS_model, train_dl)))
              print('Epoch: %03d/%03d test accuracy: %.2f%%' % (epoch+1, num_epochs, compute_accuracy(NAS_model, test_dl)))
              #append the accuracy for each epoch
              training_accuracy_array.append(training_accuracy)
              test_accuracy_array.append(test_accuracy)
    
          # architecture['Train loss'].append(cost)
          # architecture['Test loss'].append(test_cost)
          # architecture['Training accuracy'].append(training_accuracy)
          # architecture['Test accuracy'].append(test_accuracy)
    
          # print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))
          
      # plt.figure()
      # plt.plot(training_accuracy_array, '-.r',linewidth= 3)
      # plt.plot(test_accuracy_array, 'ob', linewidth = 1)
      # plt.legend(['Training accuracy', 'Test accuracy'])
      # plt.xlabel('Epochs')
      # plt.ylabel('Accuracy')
      # plt.show()
      return cost, test_cost, training_accuracy, test_accuracy, train_loss, test_loss, training_accuracy_array, test_accuracy_array
    
    # Monitoring training time of the net
    ## Training
    start_time = time.time()
    start_time_for_total = time.time()
    
    
    def initial_netarch_setup(img_channels, num_classes, b, p_b1, p_b2, p_b3, p_b4):
      N_model_samples = 10
    
      m1 = Categorical(p_b1)
      m2 = Categorical(p_b2)
      m3 = Categorical(p_b3)
      m4 = Categorical(p_b4)
    
      action1 = m1.sample([N_model_samples, 1])
      action2 = m2.sample([N_model_samples, 1])
      action3 = m3.sample([N_model_samples, 1])
      action4 = m4.sample([N_model_samples, 1])  
    
      bb_1 = b[action1]
      bb_2 = b[action2]
      bb_3 = b[action3]
      bb_4 = b[action4]
    
      bb_1.to(device)
      bb_2.to(device)
      bb_3.to(device)
      bb_4.to(device)
    
      # Train the 10 sample networks
      NAS_models = torch.nn.ModuleList([])
      # NAS_model = []
      for i in range(0,N_model_samples):
        NAS_model = ResNet(block, [bb_1[i], bb_2[i], bb_3[i], bb_4[i]] , img_channels, num_classes)
        NAS_model = NAS_model.to(device)
        NAS_models.append(NAS_model)
    
      return NAS_models, bb_1, bb_2, bb_3, bb_4, p_b1, p_b2, p_b3, p_b4
    
    
    def super_Architecture(img_channels, num_classes, num_epochs, num_epochs_NAS):
       # Variables
      alpha2 = 0.002 # Update step size in equation 2 from paper. Initial value. Just a guess. Try different values.
      b = torch.tensor([2,3,4,5,6])
      normal_dist = Normal(torch.tensor([0], dtype = float), torch.tensor([1], dtype = float))
    
      p_b1 = torch.transpose(torch.nn.functional.softmax(normal_dist.sample([len(b)]), dim = 0),0,1)
      p_b2 = torch.transpose(torch.nn.functional.softmax(normal_dist.sample([len(b)]), dim = 0),0,1)
      p_b3 = torch.transpose(torch.nn.functional.softmax(normal_dist.sample([len(b)]), dim = 0),0,1)
      p_b4 = torch.transpose(torch.nn.functional.softmax(normal_dist.sample([len(b)]), dim = 0),0,1)
      
      p_b1 = p_b1.to(device)
      p_b2 = p_b2.to(device)
      p_b3 = p_b3.to(device)
      p_b4 = p_b4.to(device)
    
      NAS_models, bb_1, bb_2, bb_3, bb_4, p_b1, p_b2, p_b3, p_b4 = initial_netarch_setup(img_channels, num_classes, b, p_b1, p_b2, p_b3, p_b4)
      print('********************')
      print('all start b')
      print([bb_1, bb_2, bb_3, bb_4])
      print('all start p')
      print([p_b1, p_b2, p_b3, p_b4])
      print('********************')
    
    
      start_time = time.time()
    
      NAS_models_best = []
      bbbb_best = []
      test_accuracy_pro = []
      training_accuracy_pro = []
      bbbb_progression = []
      probability_progression = []
      for epoch in range(num_epochs_NAS):
        cur_epoch_nas = epoch+1
        for j in range(0, len(NAS_models)):
          print()
          print('Model sample',j+1)
          NAS_models_progression = []
          Test_accuracy_best = []
          Train_accuracy_best = []
          logits_ten_models_b1 = []
          logits_ten_models_b2 = []
          logits_ten_models_b3 = []
          logits_ten_models_b4 = []
          # test_accuracy_pro = []
          # training_accuracy_pro = []
          # bbbb_progression = []
          # probability_progression = []
          NAS_model = NAS_models[j]
          b_1 = bb_1[j]
          b_2 = bb_2[j]
          b_3 = bb_3[j]
          b_4 = bb_4[j]
          # p_1 = p_b1[j]
          # p_2 = p_b2[j]
          # p_3 = p_b3[j]
          # p_4 = p_b4[j]
          optimizer = torch.optim.Adam(NAS_model.parameters(), lr=learning_rate)
      
    #      for epoch in range(num_epochs_NAS):
          bbbb = [b_1, b_2, b_3, b_4]
          probabilities = [p_b1, p_b2, p_b3, p_b4]
          #probability_progression.append(probabilities)
          #bbbb_progression.append(bbbb)
          #print('Epoch sample',epoch+1)
          #print("Jeg er her 1")
          cost, test_cost, training_accuracy, test_accuracy, train_loss, test_loss, training_accuracy_array, test_accuracy_array = trainmodel(NAS_model, optimizer)
    
    
          test_accuracy_pro.append(test_accuracy_array)
          training_accuracy_pro.append(training_accuracy_array)
          NAS_models_progression.append(NAS_model)
          Test_accuracy_best.append(test_accuracy_array[-1])
          Train_accuracy_best.append(training_accuracy_array[-1])
          
          #print('Test accuracy', Test_accuracy_best)
          #print('Train accuracy', Train_accuracy_best)
    
          #print('Test accuracy', test_accuracy_array)
          #print('Train accuracy', training_accuracy_array)
    
          #print(tabulate([[np.ones(range(1,num_epochs_NAS))*j+1], [probability_progression], [bbbb_progression], [Test_accuracy_best], [Train_accuracy_best]], headers=['Model #','Probabilities', 'Network variables', 'Test_accuracy','Train_accuracy'], tablefmt='orgtbl'))
        
    
          #test_accuracy_pro = torch.stack(test_accuracy_pro)
          # = torch.stack(training_accuracy_pro)
    
    
          # plt.figure()
          # plt.plot(training_accuracy_pro, '-.',linewidth= 1)
          # plt.plot(test_accuracy_pro, 'o', linewidth = 1)
          # plt.plot(training_accuracy_pro, '-.',linewidth= 3, label = 'train # %s'%j)
          # plt.plot(test_accuracy_pro, 'o', linewidth = 1, label = 'test # %s'%j)
          # plt.legend('Train acc','Test acc')
          # plt.xlabel('Epochs')
          # plt.ylabel('Accuracy')
          # plt.show()
    
        # Now we convert the above probability to a logit via softmax, such that we can easily compute the gradient as probability-1. 
        # he logit is a type of function that maps probability values from {\displaystyle (0,1)}(0,1) to real numbers in {\displaystyle (-\infty ,+\infty )}{\displaystyle (-\infty ,+\infty )}
    
        # We will use the logit as our log likelihood - Christoffer Riis said this at the lecture.  
    
        # Hvis logits_ er vores loss function altså gradient_b*log(pi(a|b)) så er b_l1-1 som er vores gradient, som skal være med i summen fra ligning 2. Sigma er softmax.
    
        # Use optimizer.step() to update the weights              
        logits_b1 = torch.log(p_b1)
        logits_b2 = torch.log(p_b2)
        logits_b3 = torch.log(p_b3)
        logits_b4 = torch.log(p_b4)
    
        logits_ten_models_b1.append(logits_b1)
        logits_ten_models_b2.append(logits_b2)
        logits_ten_models_b3.append(logits_b3)
        logits_ten_models_b4.append(logits_b4)
    
        logits_ten_models_b1 = torch.stack(logits_ten_models_b1)
        logits_ten_models_b2 = torch.stack(logits_ten_models_b2)
        logits_ten_models_b3 = torch.stack(logits_ten_models_b3)
        logits_ten_models_b4 = torch.stack(logits_ten_models_b4)
    
        Test_accuracy_best = torch.stack(Test_accuracy_best)
        Test_accuracy_best = Test_accuracy_best.to(device)
    
        Best_accuracy_idx = torch.argmax(Test_accuracy_best)
        Best_accuracy = Test_accuracy_best[Best_accuracy_idx]
    
    
        # Defining indicator variable   
        I = torch.zeros(len(b))
        I[Best_accuracy_idx] = 1
        I = I.to(device)
    
        # best_logits_pb1 = logits_ten_models_b1[Best_accuracy_idx]
        # best_logits_pb2 = logits_ten_models_b2[Best_accuracy_idx]
        # best_logits_pb3 = logits_ten_models_b3[Best_accuracy_idx]
        # best_logits_pb4 = logits_ten_models_b4[Best_accuracy_idx]
        
        # print('bs for model %s' %j)
        # print(bbbb_progression)
        # print('probabilities for model %s' %j)
        # print(probability_progression)
        # print('')
        # this is our probability for our bs - this is our gradient step
      
        #print('Torch test accuracy', Test_accuracy_best)
    
        p_b1 = p_b1*I + alpha2*torch.mean(Test_accuracy_best)*torch.mean((1-logits_b1))*I
        p_b2 = p_b2*I + alpha2*torch.mean(Test_accuracy_best)*torch.mean((1-logits_b2))*I
        p_b3 = p_b3*I + alpha2*torch.mean(Test_accuracy_best)*torch.mean((1-logits_b3))*I
        p_b4 = p_b4*I + alpha2*torch.mean(Test_accuracy_best)*torch.mean((1-logits_b4))*I
        # p_b1 = p_b1 + alpha2*torch.mean(Test_accuracy_best)*torch.mean((logits_b1-1))*I
        # p_b2 = p_b2 + alpha2*torch.mean(Test_accuracy_best)*torch.mean((logits_b2-1))*I
        # p_b3 = p_b3 + alpha2*torch.mean(Test_accuracy_best)*torch.mean((logits_b3-1))*I
        # p_b4 = p_b4 + alpha2*torch.mean(Test_accuracy_best)*torch.mean((logits_b4-1))*I
    
        #print('Best accuracy', Best_accuracy)
        # p_b1 = p_b1 + alpha2*Best_accuracy*torch.mean((logits_b1-1))
        # p_b2 = p_b2 + alpha2*Best_accuracy*torch.mean((logits_b2-1))
        # p_b3 = p_b3 + alpha2*Best_accuracy*torch.mean((logits_b3-1))
        # p_b4 = p_b4 + alpha2*Best_accuracy*torch.mean((logits_b4-1))
    
    
        # Normalize our probabilities to sum to 1
        p_b1 = torch.nn.functional.softmax(p_b1, dim = 1)
        p_b2 = torch.nn.functional.softmax(p_b2, dim = 1)
        p_b3 = torch.nn.functional.softmax(p_b3, dim = 1)
        p_b4 = torch.nn.functional.softmax(p_b4, dim = 1)
        
      
      
        NAS_models, bb_1, bb_2, bb_3, bb_4, p_b1, p_b2, p_b3, p_b4 = initial_netarch_setup(img_channels, num_classes, b, p_b1, p_b2, p_b3, p_b4)
    
        
    
        probability_progression.append([p_b1, p_b2, p_b3, p_b4])
        bbbb_progression.append([bb_1, bb_2, bb_3, bb_4])
        
    
    
    
        print('bs after update # %s' % (cur_epoch_nas))
        #print([bb_1, bb_2, bb_3, bb_4])
        print('ps after update # %s' % (cur_epoch_nas))
        #print([p_b1, p_b2, p_b3, p_b4])
    
    
        # print('mean test acc', str(torch.mean(Test_accuracy_best)))
        # print()
        # print('bbbb',bbbb_progression)
    
      # print('bs for model %s/3' %(j+1))
      # print(bbbb_progression)
      # print('probabilities for model %s/3' %(j+1))
      # print(probability_progression)
      # print('')
    
        print('Total time elapsed for super architecture: %.2f min' % ((time.time() - start_time_for_total)/60))
        
      print('all bs', bbbb_progression)
      print('all probs', probability_progression)
      print('all test accs', test_accuracy_pro)
      print('all train acc', training_accuracy_pro)
        # np.savetxt('bs.txt', bbbb_progression)
        # np.savetxt('ps.txt', probability_progression)
        # np.savetxt('test_acc.txt', test_accuracy_pro.numpy())
        # np.savetxt('train_acc.txt', training_accuracy_pro.numpy())
    
      return 
    
    
    
    # notebook.list()
    
    NAS_test = super_Architecture(img_channels, num_classes, num_epochs, num_epochs_NAS)
  
    
    
    # # Attempt from https://github.com/rasbt/stat453-deep-learning-ss21/blob/main/L14/2-resnet-example.ipynb
    
    # torch.manual_seed(random_seed)
    # model = ResNet50(img_channels, num_classes)
    # model = model.to(device)
    # #model = model.to("cuda")
    
    # loss_fn=nn.CrossEntropyLoss()    
    # optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
    
    # trainedmodel = trainmodel(model, optimizer)